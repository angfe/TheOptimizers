{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilization Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charging_sessions_data = pd.read_csv(\"cleaned_data.csv\")\n",
    "weather_data = pd.read_csv(\"cleaned_weather_data.csv\")\n",
    "\n",
    "charging_sessions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charging_sessions_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the export to csv file the right format cannot be kept\n",
    "# Therefore, the columns with datetime type need to be reformatted\n",
    "charging_sessions_data[\"connectionTime\"] = pd.to_datetime(charging_sessions_data[\"connectionTime\"])\n",
    "charging_sessions_data[\"disconnectTime\"] = pd.to_datetime(charging_sessions_data[\"disconnectTime\"])\n",
    "charging_sessions_data[\"doneChargingTime\"] = pd.to_datetime(charging_sessions_data[\"doneChargingTime\"])\n",
    "weather_data[\"timestamp\"] = pd.to_datetime(weather_data[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing Prediction Model on hourly Utilization -> connectiontime to disconnectiontime in hours\n",
    "1. **Feature Engineering**\n",
    "* Normalizing and standardizing data **(Angela)**\n",
    "* Feature encoding (transforming categorical values into numerical values) **(Coco)**\n",
    "* Determining features (Correlation) **(Marietta)**\n",
    "* Join charging sessions and weather data **(Coco)**\n",
    "2. **Find Optimal Machine Learning Method**\n",
    "* Test different ML methods and evaluate them based on one metrics\n",
    "    * Polynomial Regression **(Simon)**\n",
    "    * Lasso Regression **(Simon)**\n",
    "    * Ridge Regression **(Marietta)**\n",
    "    * Random Forest **(Angela)**\n",
    "    * Neural Network **(Coco)**\n",
    "* Choose the ML methods demonstrating the best performance (Ridge Regression)\n",
    "3. **Developing Predictive Models**\n",
    "* Prediction models are developed for its charging site\n",
    "* Develop predictive model using neural networks\n",
    "* Develop predictive model using any other machine learning methods of choice (see Section 1.)\n",
    "* Use cross-validation to train the models\n",
    "* Compare predictive performance of both models on the same holdout set\n",
    "* Determine the type of model the operator should employ\n",
    "4. **Examples for Business Case**\n",
    "* Visualize data prediction to support/ enables business case\n",
    "* Make example predictions to support/ enables business case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Data Standardization and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing and normalizing the data is important to improve the model performance since most models assume that the features are on a similar scale. The choice between standardization and normalization is dependent on the machine learning model. \\\n",
    "We have decided to standardize the data for Polynomial Regression, Lasso and Ridge Regression. Neural Networks can use either options, but normalization is often preferred. Thus, we will apply normalization to its data. \\\n",
    "Random Forest and other tree-based models are inherently insensitive to the scale of the features, so scaling the dataset does not impact performance. Therefore its training data does not require any scaling. \\\n",
    "It is important to note that the data should be scaled after splitting the dataset into training and test data to prevent data leakage and preserve the validity of the test results. Thus, this part demonstrate how data can be scaled accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.1. Charging Session Data Standardization and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charging_sessions_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charging_sessions_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop non-numerical data\n",
    "scaled_charging_sessions_data = charging_sessions_data[['kWhDelivered', 'WhPerMile',\n",
    "       'kWhRequested', 'milesRequested', 'minutesAvailable']]\n",
    "scaled_charging_sessions_data.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 is for standardized data and X2 is for normalized data\n",
    "X1 = scaled_charging_sessions_data\n",
    "X2 = scaled_charging_sessions_data\n",
    "\n",
    "# standardize data\n",
    "scaleStandard = StandardScaler()\n",
    "X1 = scaleStandard.fit_transform(X1)\n",
    "X1 = pd.DataFrame(X1, columns=['kWhDelivered', 'WhPerMile', 'kWhRequested', 'milesRequested', 'minutesAvailable'])\n",
    "X1.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "scaleMinMax = MinMaxScaler(feature_range=(0, 1))\n",
    "X2 = scaleMinMax.fit_transform(X2)\n",
    "X2 = pd.DataFrame(X2, columns=['kWhDelivered', 'WhPerMile', 'kWhRequested', 'milesRequested', 'minutesAvailable'])\n",
    "X2.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.2. Weather Data Standardization and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop non-numerical data\n",
    "scaled_weather_data = weather_data[['temperature', 'cloud_cover', 'pressure', 'windspeed', 'precipitation',\n",
    "       'felt_temperature']]\n",
    "scaled_charging_sessions_data.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 is for standardized data and X2 is for normalized data\n",
    "X11 = scaled_weather_data\n",
    "X22 = scaled_weather_data\n",
    "\n",
    "# standardize data\n",
    "scaleStandard = StandardScaler()\n",
    "X11 = scaleStandard.fit_transform(X11)\n",
    "X11 = pd.DataFrame(X11, columns=['temperature', 'cloud_cover', 'pressure', 'windspeed', 'precipitation',\n",
    "       'felt_temperature'])\n",
    "X11.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "scaleMinMax = MinMaxScaler(feature_range=(0, 1))\n",
    "X22 = scaleMinMax.fit_transform(X22)\n",
    "X22 = pd.DataFrame(X22, columns=['temperature', 'cloud_cover', 'pressure', 'windspeed', 'precipitation',\n",
    "       'felt_temperature'])\n",
    "X22.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Feature Enconding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding weekday \n",
    "weather_data[\"is_weekday\"] = np.where(weather_data['timestamp'].dt.weekday.isin([5, 6]), 0, 1)\n",
    "charging_sessions_data[\"is_weekday\"] = np.where(charging_sessions_data['connectionTime'].dt.weekday.isin([5, 6]), 0, 1)\n",
    "\n",
    "# Encoding siteID\n",
    "charging_sessions_data[\"siteID\"] = np.where(charging_sessions_data['siteID'] == 1, 1, 0)\n",
    "\n",
    "# Extract hour and month\n",
    "weather_data[\"month\"] = weather_data['timestamp'].dt.month\n",
    "weather_data[\"hour\"] = weather_data['timestamp'].dt.hour\n",
    "charging_sessions_data[\"month\"] = charging_sessions_data['connectionTime'].dt.month\n",
    "charging_sessions_data[\"hour\"] = charging_sessions_data['connectionTime'].dt.hour\n",
    "\n",
    "# Encoding hour by Sin-Cos Encoding\n",
    "charging_sessions_data['hour_sin'] = np.sin(2 * np.pi * charging_sessions_data['hour'] / 24)\n",
    "charging_sessions_data['hour_cos'] = np.cos(2 * np.pi * charging_sessions_data['hour'] / 24)\n",
    "\n",
    "# Encoding month by Sin-Cos Encoding\n",
    "\n",
    "charging_sessions_data['month_sin'] = np.sin(2 * np.pi * charging_sessions_data['month'] / 12)\n",
    "charging_sessions_data['month_cos'] = np.cos(2 * np.pi * charging_sessions_data['month'] / 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets group the table by month, hour and weekday to later join it in the charging data\n",
    "# Therefore other attributes have to be averaged \n",
    "grouped_weather = ( weather_data\n",
    ".groupby(['month', 'is_weekday', 'hour'])\n",
    ".agg({\n",
    "    'temperature': 'mean',   # Average temperature\n",
    "    'precipitation': 'mean',      # Average precipitation\n",
    "    'windspeed': 'mean', # Average windspeed\n",
    "    'cloud_cover': 'mean', # Average cloud cover\n",
    "    'pressure': 'mean'    # Average pressure\n",
    "}).reset_index())\n",
    "grouped_weather.rename(columns = {\"temperature\" : \"avg_temperature\", \"precipitation\": \"avg_precipitation\",\"windspeed\":\"avg_windspeed\",\"cloud_cover\":\"avg_cloud\",\"pressure\":\"avg_pressure\"}, inplace = True)\n",
    "grouped_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Join charging sessions and weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we merge weather data to charging session data on month, hour and is_weekday\n",
    "\n",
    "merged_sessions = charging_sessions_data.merge(grouped_weather, on = [\"month\", \"is_weekday\", \"hour\"] )\n",
    "merged_sessions = merged_sessions.drop(['spaceID','stationID','timezone','userID', 'userInputs'], axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the session data contain all the recorded charging sessions with an addition of weather information, we have to think about how to map these to the target variable Utilization (each entry has to have a target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sessions.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Determining Features (Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charging_sessions_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of relecant numeric features \n",
    "correlation_features = [\n",
    "    'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'is_weekday',\n",
    "    'avg_temperature', 'avg_windspeed',  'avg_precipitation', 'avg_pressure', 'avg_cloud',\n",
    "    'WhPerMile', 'kWhRequested', 'milesRequested', 'minutesAvailable', 'doneChargingTime'  \n",
    "]\n",
    "\n",
    "# calculate the correlation \n",
    "correlation_matrix = merged_sessions[correlation_features].corr()\n",
    "\n",
    "# plot a heatmap of the correlation \n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix: Charging Sessions and Weather Data')\n",
    "plt.show()\n",
    "### 2. Find Optimal Machine Learning Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find Optimal Machine Learning Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Predicting Utilization Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the stations for each siteID \n",
    "stations_per_site = charging_sessions_data.groupby('siteID')['stationID'].nunique().reset_index()\n",
    "stations_per_site.rename(columns={'stationID': 'total_stations'}, inplace=True)\n",
    "\n",
    "print(stations_per_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the duration of each session\n",
    "charging_sessions_data['connectionTime'] = pd.to_datetime(charging_sessions_data['connectionTime'])\n",
    "charging_sessions_data['disconnectTime'] = pd.to_datetime(charging_sessions_data['disconnectTime'])\n",
    "charging_sessions_data['duration'] = (charging_sessions_data['disconnectTime'] - charging_sessions_data['connectionTime']).dt.total_seconds() / 3600  # duration in hours\n",
    "\n",
    "# Summarise the loading times per hour and site\n",
    "hourly_utilization = charging_sessions_data.groupby(['connectionTime', 'siteID']).agg(\n",
    "    active_time=('duration', 'sum'),  # sum of the loading time in hours\n",
    ").reset_index()\n",
    "\n",
    "# Add the total number of stations for each site\n",
    "hourly_utilization = hourly_utilization.merge(stations_per_site, on='siteID', how='left')\n",
    "\n",
    "# Calculate the utilization per hour\n",
    "hourly_utilization['utilization'] = hourly_utilization['active_time'] / (hourly_utilization['total_stations'])\n",
    "\n",
    "#Check the first five results\n",
    "print(hourly_utilization)\n",
    "\n",
    "# prediciting utilization value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sessions = merged_sessions.merge(\n",
    "    hourly_utilization[['connectionTime', 'siteID', 'utilization']],  \n",
    "    on=['connectionTime', 'siteID'],  # same columsn for the merge\n",
    "    how='left'  # all columns from charging session data should remain \n",
    ")\n",
    "# check\n",
    "merged_sessions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sessions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of relevant numeric features \n",
    "correlation_features = [\n",
    "    'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'is_weekday',\n",
    "    'avg_temperature', 'avg_windspeed',  'avg_precipitation', 'avg_pressure', 'avg_cloud',\n",
    "    'WhPerMile', 'kWhRequested', 'milesRequested', 'minutesAvailable', 'doneChargingTime',\n",
    "    'session_duration', 'idle_time', 'energy_ratio', 'kWhDelivered', 'utilization' \n",
    "]\n",
    "\n",
    "# calculate the correlation \n",
    "correlation_matrix = merged_sessions[correlation_features].corr()\n",
    "\n",
    "# plot a heatmap of the correlation \n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix: Charging Sessions and Weather Data')\n",
    "plt.show()\n",
    "### 2. Find Optimal Machine Learning Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sessions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy (not strictly required, but good practice)\n",
    "merged_sessions_poly = merged_sessions.copy()\n",
    "\n",
    "# Drop rows with NaN in session_duration, idle_time, or utilization\n",
    "cols_needed = ['session_duration', 'idle_time', 'utilization']\n",
    "merged_sessions_poly = merged_sessions_poly.dropna(subset=cols_needed)\n",
    "\n",
    "# Select features and target\n",
    "X_poly = merged_sessions_poly[['session_duration', 'idle_time']]\n",
    "y_poly = merged_sessions_poly['utilization']\n",
    "\n",
    "# 60/20/20 split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_poly, \n",
    "    y_poly, \n",
    "    test_size=0.4,   # 40% for test+holdout\n",
    "    random_state=42\n",
    ")\n",
    "X_test, X_holdout, y_test, y_holdout = train_test_split(\n",
    "    X_temp, \n",
    "    y_temp, \n",
    "    test_size=0.5,   # split temp 50/50 => 20/20\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create polynomial features (degree=2, no bias)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "X_holdout_poly = poly.transform(X_holdout)\n",
    "\n",
    "# Fit a linear model on the polynomial-transformed features\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predict on train, test, holdout\n",
    "y_pred_train = poly_model.predict(X_train_poly)\n",
    "y_pred_test = poly_model.predict(X_test_poly)\n",
    "y_pred_holdout = poly_model.predict(X_holdout_poly)\n",
    "\n",
    "# Evaluate metrics for train, test, holdout\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "holdout_r2 = r2_score(y_holdout, y_pred_holdout)\n",
    "holdout_mse = mean_squared_error(y_holdout, y_pred_holdout)\n",
    "\n",
    "# Print results\n",
    "print(\"Polynomial Regression with 'session_duration' and 'idle_time' -> Target: 'utilization'\")\n",
    "print(f\"Train R²: {train_r2:.4f}\")\n",
    "print(f\"Train MSE: {train_mse:.6f}\")\n",
    "print(f\"Test R²:  {test_r2:.4f}\")\n",
    "print(f\"Test MSE:  {test_mse:.6f}\")\n",
    "print(f\"Holdout R²: {holdout_r2:.4f}\")\n",
    "print(f\"Holdout MSE: {holdout_mse:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of your DataFrame (optional, but good practice)\n",
    "merged_sessions_lasso = merged_sessions.copy()\n",
    "\n",
    "# Drop rows that have NaN in any relevant columns:\n",
    "#    We need 'session_duration', 'idle_time', AND 'utilization' to be valid\n",
    "needed_cols = ['session_duration', 'idle_time', 'utilization']\n",
    "merged_sessions_lasso = merged_sessions_lasso.dropna(subset=needed_cols)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_lasso = merged_sessions_lasso[['session_duration', 'idle_time']]\n",
    "y_lasso = merged_sessions_lasso['utilization']\n",
    "\n",
    "# Split into train (60%), test (20%), holdout (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_lasso, \n",
    "    y_lasso, \n",
    "    test_size=0.4,  # 40% for test+holdout\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_test, X_holdout, y_test, y_holdout = train_test_split(\n",
    "    X_temp, \n",
    "    y_temp, \n",
    "    test_size=0.5,  # split temp into 20% / 20%\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Standardize (fit on training, transform all)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_holdout_scaled = scaler.transform(X_holdout)\n",
    "\n",
    "# Perform Lasso + GridSearch for alpha\n",
    "lasso = Lasso(max_iter=10000, random_state=42)\n",
    "\n",
    "#    alpha from 1e-4 to 10, ~50 values\n",
    "param_grid = {'alpha': np.logspace(-4, 1, 50)}  \n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lasso, \n",
    "    param_grid=param_grid, \n",
    "    scoring='r2', \n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Retrieve best alpha\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(\"Best Alpha:\", best_alpha)\n",
    "\n",
    "# Fit final Lasso model\n",
    "best_lasso = Lasso(alpha=best_alpha, max_iter=10000, random_state=42)\n",
    "best_lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = best_lasso.predict(X_train_scaled)\n",
    "y_pred_test = best_lasso.predict(X_test_scaled)\n",
    "y_pred_holdout = best_lasso.predict(X_holdout_scaled)\n",
    "\n",
    "# Evaluate on Train, Test, Holdout\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "holdout_r2 = r2_score(y_holdout, y_pred_holdout)\n",
    "holdout_mse = mean_squared_error(y_holdout, y_pred_holdout)\n",
    "\n",
    "print(\"Lasso Regression with 'session_duration' and 'idle_time' -> 'utilization'\")\n",
    "print(f\"Best Alpha: {best_alpha}\")\n",
    "print(f\"Train R²: {train_r2:.4f} | MSE: {train_mse:.6f}\")\n",
    "print(f\"Test  R²: {test_r2:.4f}  | MSE: {test_mse:.6f}\")\n",
    "print(f\"Holdout R²: {holdout_r2:.4f} | MSE: {holdout_mse:.6f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep numeric data for the regression\n",
    "merged_sessionsRidge = merged_sessions.select_dtypes(include=['number']) \n",
    "print(merged_sessionsRidge.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Features and Target\n",
    "# Define target variable\n",
    "target = 'utilization'\n",
    "\n",
    "# Define features \n",
    "XRidge = merged_sessionsRidge[['session_duration', 'idle_time']] # features\n",
    "yRidge = merged_sessionsRidge['utilization'] # target variable\n",
    "\n",
    "# split dataset into training (60%), testing (20%) and holdout (20%) sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(XRidge, yRidge, test_size=0.3, random_state=42)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(XRidge, yRidge, test_size=0.4, random_state=42)  # 40% temp (test + holdout)\n",
    "X_test, X_holdout, y_test, y_holdout = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split temp 50/50\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_holdout_scaled = scaler.transform(X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge-Regression Model\n",
    "ridge = Ridge(max_iter=10000, random_state=42)\n",
    "\n",
    "# Set up a grid search for Ridge with different alpha values\n",
    "param_grid = {'alpha': np.logspace(-4, 1, 50)}  # Test alpha values from 0.0001 to 10\n",
    "grid_search = GridSearchCV(ridge, param_grid, scoring='r2', cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best alpha value\n",
    "best_alphaRidge = grid_search.best_params_['alpha']\n",
    "print(f\"Best Alpha: {best_alphaRidge}\")\n",
    "\n",
    "# Train the Ridge model with the best alpha\n",
    "best_ridge = Ridge(alpha=best_alphaRidge, max_iter=10000, random_state=42)\n",
    "best_ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = best_ridge.predict(X_train_scaled)\n",
    "y_pred_test = best_ridge.predict(X_test_scaled)\n",
    "y_pred_holdout = best_ridge.predict(X_holdout_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the model\n",
    "# Evaluation on the training set\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "\n",
    "# Evaluation on the test set\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "#Evaluation on the holdout set\n",
    "holdout_r2 = r2_score(y_holdout, y_pred_holdout)\n",
    "holdout_mse = mean_squared_error(y_holdout, y_pred_holdout)\n",
    "\n",
    "# print the results\n",
    "print(\"Train R²:\", train_r2)\n",
    "print(\"Train MSE:\", train_mse)\n",
    "print(\"Test R²:\", test_r2)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Holdout R²:\", holdout_r2)\n",
    "print(\"Holdout MSE:\", holdout_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs well, with high R² values on Train (92.7%), Test (91.7%), and Holdout (89.4%) sets, indicating strong explanatory power and good generalization. \n",
    "The MSE values are consistently low, reflecting accurate predictions across all datasets. The close performance between Test and Holdout suggests the model is not overfitted and is robust for unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "# Plot predicted vs actual for test data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.7, label=\"Test Data\")\n",
    "plt.scatter(y_holdout, y_pred_holdout, alpha=0.7, label=\"Holdout Data\")\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label=\"Perfect Prediction\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_session_rf = merged_sessions\n",
    "\n",
    "# Select Features and Target\n",
    "# Define target variable\n",
    "target = 'utilization'\n",
    "\n",
    "# Define features\n",
    "X_rf = merged_session_rf[['session_duration', 'idle_time']] # features\n",
    "y_rf = merged_session_rf['utilization'] # target variable\n",
    "\n",
    "# Split dataset into training, testing and holdout sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_rf, y_rf, test_size=0.4, random_state=42)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_rf, y_rf, test_size=0.4, random_state=42)  # 40% temp (test + holdout)\n",
    "X_test, X_holdout, y_test, y_holdout = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split temp 50/50\n",
    "\n",
    "# Train model on 100 random trees on train set\n",
    "rf_model = RandomForestRegressor(n_estimators=100, bootstrap=True, random_state=42) # we select boostrapp, i.e. we use bagging\n",
    "rf_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "y_pred_holdout = rf_model.predict(X_holdout)\n",
    "\n",
    "# evaluate performance based on metrics on train and test set\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred) # good performance = close to 1\n",
    "\n",
    "# Evaluation on the training set\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "\n",
    "# Evaluation on the test set\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "#Evaluation on the holdout set\n",
    "holdout_r2 = r2_score(y_holdout, y_pred_holdout)\n",
    "holdout_mse = mean_squared_error(y_holdout, y_pred_holdout)\n",
    "\n",
    "print(\"Train R²:\", train_r2)\n",
    "print(\"Train MSE:\", train_mse)\n",
    "print(\"Test R²:\", test_r2)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Holdout R²:\", holdout_r2)\n",
    "print(\"Holdout MSE:\", holdout_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200], # no of trees\n",
    "    'max_depth': [5, 10, 20], # depth of each tree\n",
    "    'min_samples_split': [1, 2, 5], # no required to split\n",
    "    'min_samples_leaf': [1, 2, 4] # min samples required to node\n",
    "}\n",
    "\n",
    "# training of holdout set\n",
    "rf_model_cv = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "rf_model_cv.fit(X_holdout, y_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", rf_model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_train = rf_model_cv.predict(X_train)\n",
    "y_pred_test = rf_model_cv.predict(X_test)\n",
    "y_pred_holdout = rf_model_cv.predict(X_holdout)\n",
    "\n",
    "# evaluate performance based on metrics on train and test set\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred) # good performance = close to 1\n",
    "\n",
    "# Evaluation on the training set\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "\n",
    "# Evaluation on the test set\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "#Evaluation on the holdout set\n",
    "holdout_r2 = r2_score(y_holdout, y_pred_holdout)\n",
    "holdout_mse = mean_squared_error(y_holdout, y_pred_holdout)\n",
    "\n",
    "print(\"Train R²:\", train_r2)\n",
    "print(\"Train MSE:\", train_mse)\n",
    "print(\"Test R²:\", test_r2)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Holdout R²:\", holdout_r2)\n",
    "print(\"Holdout MSE:\", holdout_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLPRegressor (Multi-Layer Perceptron Regressor) in scikit-learn is a neural network model that implements a feedforward artificial neural network with fully connected layers, trained using backpropagation.\n",
    "\n",
    "For this method we decided to include more features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Conduct train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = merged_sessions[[\"siteID\",\"WhPerMile\",\"kWhRequested\",\"minutesAvailable\",\"is_weekday\",\"session_duration\",\"idle_time\",\"avg_temperature\",\"avg_precipitation\",\"avg_windspeed\", \"hour_sin\", \"hour_cos\"]]\n",
    "y = merged_sessions['utilization']\n",
    "\n",
    "# Split into train, validation and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=123)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=123)\n",
    "\n",
    "# Scale train data\n",
    "NN_features = [\"siteID\",\"WhPerMile\",\"kWhRequested\",\"minutesAvailable\",\"is_weekday\",\"session_duration\",\"idle_time\",\"avg_temperature\",\"avg_precipitation\",\"avg_windspeed\", \"hour_sin\", \"hour_cos\"]\n",
    "X_NN_scale = X_train[[\"siteID\",\"WhPerMile\",\"kWhRequested\",\"minutesAvailable\",\"is_weekday\",\"session_duration\",\"idle_time\",\"avg_temperature\",\"avg_precipitation\",\"avg_windspeed\"]]\n",
    "X_NN_exclude = X_train[[\"hour_sin\",\"hour_cos\"]]\n",
    "\n",
    "# Scaling the chosen deatures except for hour_sin and hour_cos (these are naturally in scale)\n",
    "X_NN_scaled = scaleStandard.fit_transform(X_NN_scale)\n",
    "X_train = pd.DataFrame(\n",
    "    np.hstack([X_NN_scaled, X_NN_exclude.values]),  # Combine scaled and unscaled features\n",
    "    columns=NN_features # Keep the column names\n",
    ")\n",
    "\n",
    "# scale test data\n",
    "X_NN_scale = X_val[[\"siteID\",\"WhPerMile\",\"kWhRequested\",\"minutesAvailable\",\"is_weekday\",\"session_duration\",\"idle_time\",\"avg_temperature\",\"avg_precipitation\",\"avg_windspeed\"]]\n",
    "X_NN_exclude = X_val[[\"hour_sin\",\"hour_cos\"]]\n",
    "\n",
    "# Scaling the chosen deatures except for hour_sin and hour_cos (these are naturally in scale)\n",
    "X_NN_scaled = scaleStandard.fit_transform(X_NN_scale)\n",
    "X_val = pd.DataFrame(\n",
    "    np.hstack([X_NN_scaled, X_NN_exclude.values]),  # Combine scaled and unscaled features\n",
    "    columns=NN_features # Keep the column names\n",
    ")\n",
    "\n",
    "# scale holdout data\n",
    "X_NN_scale = X_test[[\"siteID\",\"WhPerMile\",\"kWhRequested\",\"minutesAvailable\",\"is_weekday\",\"session_duration\",\"idle_time\",\"avg_temperature\",\"avg_precipitation\",\"avg_windspeed\"]]\n",
    "X_NN_exclude = X_test[[\"hour_sin\",\"hour_cos\"]]\n",
    "\n",
    "# Scaling the chosen deatures except for hour_sin and hour_cos (these are naturally in scale)\n",
    "X_NN_scaled = scaleStandard.fit_transform(X_NN_scale)\n",
    "X_test = pd.DataFrame(\n",
    "    np.hstack([X_NN_scaled, X_NN_exclude.values]),  # Combine scaled and unscaled features\n",
    "    columns=NN_features # Keep the column names\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (100,50), (100, 50)],  # Different layer sizes\n",
    "    'activation': ['relu', 'tanh'],  # Different activation functions\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],  # Different learning rates\n",
    "}\n",
    "mlp = MLPRegressor(\n",
    "    max_iter=100,     \n",
    ")\n",
    "\n",
    "# Grid search with 5-fold cross-validation on the training set\n",
    "grid_search = GridSearchCV(mlp, param_grid, scoring='r2', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the hyperparameters with the best parameters\n",
    "best_mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "    activation=best_params['activation'],\n",
    "    learning_rate_init=best_params['learning_rate_init'],\n",
    "    max_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "best_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = best_mlp.predict(X_train)\n",
    "y_pred_val = best_mlp.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "val_r2 = r2_score(y_val, y_pred_val)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "\n",
    "print(\"1. Validation\")\n",
    "print(\"Train R²:\", train_r2)\n",
    "print(\"Train MSE:\", train_mse)\n",
    "print(\"Validation R²:\", val_r2)\n",
    "print(\"Validation MSE:\", val_mse)\n",
    "\n",
    "# Tuning around best parameters\n",
    "fine_tuned_param_grid = {\n",
    "    'hidden_layer_sizes': [(100,10),(100,20),(100,30),(100,40),(100,50),(100,60),(100,70)],\n",
    "    'learning_rate_init': [best_params['learning_rate_init'] / 2, best_params['learning_rate_init'], best_params['learning_rate_init'] * 2],\n",
    "}\n",
    "grid_search_fine = GridSearchCV(mlp, fine_tuned_param_grid, scoring='r2', cv=5)\n",
    "grid_search_fine.fit(X_train, y_train)\n",
    "better_params = grid_search_fine.best_params_\n",
    "print(f\"Fine-Tuned Best Parameters: {better_params}\")\n",
    "\n",
    "tuned_mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=better_params['hidden_layer_sizes'],\n",
    "    activation=best_params['activation'],\n",
    "    learning_rate_init=better_params['learning_rate_init'],\n",
    "    max_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tuned_mlp.fit(X_train, y_train) \n",
    "\n",
    "# Predictions\n",
    "y_pred_train = tuned_mlp.predict(X_train)\n",
    "y_pred_val = tuned_mlp.predict(X_val)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "val_r2 = r2_score(y_val, y_pred_val)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "\n",
    "print(\"2. Validation\")\n",
    "print(\"Train R²:\", train_r2)\n",
    "print(\"Train MSE:\", train_mse)\n",
    "print(\"Validation R²:\", val_r2)\n",
    "print(\"Validation MSE:\", val_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "y_pred_test = tuned_mlp.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"Holdout R²:\", test_r2)\n",
    "print(\"Holdout MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_val, y_pred_val , alpha=0.7, label=\"Testing Data\")\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.7, label=\"Holdout Data\")\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label=\"Perfect Prediction\")\n",
    "\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Neural Network: Actual vs Predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Developing Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Examples for Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Visualizing Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulate predicted utilization data for a single site over 24 hours\n",
    "#    In a real scenario, you'd have actual predictions from your trained model.\n",
    "np.random.seed(42)\n",
    "hours = np.arange(24)\n",
    "# Let's assume predicted utilization values range between 0.0 and 1.0 (0% to 100%)\n",
    "predicted_utilization = np.random.normal(loc=0.5, scale=0.2, size=24)\n",
    "predicted_utilization = np.clip(predicted_utilization, 0, 1)  # Ensure within [0,1]\n",
    "\n",
    "# Define a function to decide dynamic pricing based on predicted utilization\n",
    "#    For instance, if utilization >= 70%, we set a \"peak\" price, else an \"off-peak\" price.\n",
    "def determine_price(util):\n",
    "    if util >= 0.7:\n",
    "        return 0.40  # Example: 0.40 $/kWh at peak\n",
    "    else:\n",
    "        return 0.25  # Example: 0.25 $/kWh off-peak\n",
    "\n",
    "prices = [determine_price(u) for u in predicted_utilization]\n",
    "\n",
    "# Combine into a DataFrame for easy manipulation\n",
    "data = {\n",
    "    \"hour\": hours,\n",
    "    \"predicted_utilization\": predicted_utilization,\n",
    "    \"price_per_kWh\": prices\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Show how the price changes based on predicted utilization\n",
    "#    We’ll mark hours that exceed a threshold (e.g., 70%).\n",
    "peak_threshold = 0.7\n",
    "df[\"is_peak\"] = df[\"predicted_utilization\"] >= peak_threshold\n",
    "\n",
    "# Visualize in a bar chart: hour vs. predicted utilization\n",
    "fig, ax1 = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# Plot utilization bars\n",
    "colors = df[\"is_peak\"].map({True: \"salmon\", False: \"skyblue\"})\n",
    "ax1.bar(df[\"hour\"], df[\"predicted_utilization\"], color=colors, alpha=0.7, label=\"Predicted Utilization\")\n",
    "ax1.set_xlabel(\"Hour of the Day\")\n",
    "ax1.set_ylabel(\"Predicted Utilization (0.0–1.0)\")\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.set_xticks(range(24))\n",
    "ax1.set_title(\"Predicted Hourly Utilization & Dynamic Pricing\")\n",
    "\n",
    "# We'll add a horizontal line to indicate the threshold\n",
    "ax1.axhline(y=peak_threshold, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Peak Threshold (0.7)\")\n",
    "\n",
    "# Plot a second axis for the Price over the same hours\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df[\"hour\"], df[\"price_per_kWh\"], color=\"green\", marker=\"o\", label=\"Dynamic Price/kWh\")\n",
    "ax2.set_ylabel(\"Price ($/kWh)\")\n",
    "\n",
    "# Build a combined legend\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(h1 + h2, l1 + l2, loc=\"upper left\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print out the DataFrame for clarity\n",
    "print(\"Hourly Predicted Utilization & Dynamic Price:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Example Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
